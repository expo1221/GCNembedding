{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeac9e4-34b3-4121-9e42-6e10c1bb867b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190469b-5f18-4ca6-a97c-135b5813fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "!pip install PyTDC\n",
    "!pip install torcheval\n",
    "!pip install hyppo\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a241d7d-fc71-4a37-8a2c-caf53a3d2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, MACCSkeys\n",
    "\n",
    "from hyppo.independence import Dcorr, MGC\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "#from scipy.stats import multiscale_graphcorr as MGC\n",
    "\n",
    "from time import time\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, GCN\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torcheval\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def logistic_loss(output, target):\n",
    "  return -F.logsigmoid((2*target-1)*output) #since label is 0 and 1\n",
    "loss_function = logistic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f38716-1733-4555-b850-f18dcee090c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets HIV, HERG, CYP, TOX, BBB, HIA\n",
    "from tdc.single_pred import HTS\n",
    "data = HTS(name = 'HIV')\n",
    "HIV = data.get_split() #41,127 drugs (3.5% labeled as 1)\n",
    "\n",
    "from tdc.utils import retrieve_label_name_list\n",
    "label_list = retrieve_label_name_list('herg_central')\n",
    "from tdc.single_pred import Tox\n",
    "data = Tox(name = 'herg_central', label_name = label_list[2])\n",
    "HERG = data.get_split() #306,893 drugs (4.48% labeled as 1)\n",
    "\n",
    "from tdc.single_pred import ADME\n",
    "data = ADME(name = 'CYP2D6_Veith')\n",
    "CYP = data.get_split() #13,130 drugs (19.15% labeled as 1)\n",
    "\n",
    "from tdc.utils import retrieve_label_name_list\n",
    "label_list = retrieve_label_name_list('Tox21')\n",
    "from tdc.single_pred import Tox\n",
    "data = Tox(name = 'Tox21', label_name = label_list[0])\n",
    "TOX = data.get_split() #5086 drugs (4.253% labeled as 1)\n",
    "\n",
    "from tdc.single_pred import ADME\n",
    "data = ADME(name = 'BBB_Martins')\n",
    "BBB = data.get_split() #2030 drugs (76.4% labeled as 1)\n",
    "\n",
    "from tdc.single_pred import ADME\n",
    "data = ADME(name = 'HIA_Hou')\n",
    "HIA = data.get_split() #578 drugs (86.5% labeled as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4116424-20c5-41ca-99ff-8cd22ceedb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed to get mol\n",
    "from typing import Any, Dict, List\n",
    "x_map: Dict[str, List[Any]] = {\n",
    "    'atomic_num':\n",
    "    list(range(0, 119)),\n",
    "    'chirality': [\n",
    "        'CHI_UNSPECIFIED',\n",
    "        'CHI_TETRAHEDRAL_CW',\n",
    "        'CHI_TETRAHEDRAL_CCW',\n",
    "        'CHI_OTHER',\n",
    "        'CHI_TETRAHEDRAL',\n",
    "        'CHI_ALLENE',\n",
    "        'CHI_SQUAREPLANAR',\n",
    "        'CHI_TRIGONALBIPYRAMIDAL',\n",
    "        'CHI_OCTAHEDRAL',\n",
    "    ],\n",
    "    'degree':\n",
    "    list(range(0, 11)),\n",
    "    'formal_charge':\n",
    "    list(range(-5, 7)),\n",
    "    'num_hs':\n",
    "    list(range(0, 9)),\n",
    "    'num_radical_electrons':\n",
    "    list(range(0, 5)),\n",
    "    'hybridization': [\n",
    "        'UNSPECIFIED',\n",
    "        'S',\n",
    "        'SP',\n",
    "        'SP2',\n",
    "        'SP3',\n",
    "        'SP3D',\n",
    "        'SP3D2',\n",
    "        'OTHER',\n",
    "    ],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True],\n",
    "}\n",
    "\n",
    "e_map: Dict[str, List[Any]] = {\n",
    "    'bond_type': [\n",
    "        'UNSPECIFIED',\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'QUADRUPLE',\n",
    "        'QUINTUPLE',\n",
    "        'HEXTUPLE',\n",
    "        'ONEANDAHALF',\n",
    "        'TWOANDAHALF',\n",
    "        'THREEANDAHALF',\n",
    "        'FOURANDAHALF',\n",
    "        'FIVEANDAHALF',\n",
    "        'AROMATIC',\n",
    "        'IONIC',\n",
    "        'HYDROGEN',\n",
    "        'THREECENTER',\n",
    "        'DATIVEONE',\n",
    "        'DATIVE',\n",
    "        'DATIVEL',\n",
    "        'DATIVER',\n",
    "        'OTHER',\n",
    "        'ZERO',\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOANY',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "    ],\n",
    "    'is_conjugated': [False, True],\n",
    "}\n",
    "\n",
    "def from_smiles(smiles: str, with_hydrogen: bool = False,\n",
    "                kekulize: bool = False) -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a SMILES string to a :class:`torch_geometric.data.Data`\n",
    "    instance.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): The SMILES string.\n",
    "        with_hydrogen (bool, optional): If set to :obj:`True`, will store\n",
    "            hydrogens in the molecule graph. (default: :obj:`False`)\n",
    "        kekulize (bool, optional): If set to :obj:`True`, converts aromatic\n",
    "            bonds to single/double bonds. (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    from rdkit import Chem, RDLogger\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    RDLogger.DisableLog('rdApp.*')  # type: ignore\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    if mol is None:\n",
    "        mol = Chem.MolFromSmiles('')\n",
    "    if with_hydrogen:\n",
    "        mol = Chem.AddHs(mol)\n",
    "    if kekulize:\n",
    "        Chem.Kekulize(mol)\n",
    "\n",
    "    xs: List[List[int]] = []\n",
    "    for atom in mol.GetAtoms():  # type: ignore\n",
    "        row: List[int] = []\n",
    "        row.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
    "        row.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
    "        row.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
    "        row.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
    "        row.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
    "        row.append(x_map['num_radical_electrons'].index(\n",
    "            atom.GetNumRadicalElectrons()))\n",
    "        row.append(x_map['hybridization'].index(str(atom.GetHybridization())))\n",
    "        row.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
    "        row.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
    "        xs.append(row)\n",
    "\n",
    "    x = torch.tensor(xs, dtype=torch.long).view(-1, 9)\n",
    "\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():  # type: ignore\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = []\n",
    "        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
    "        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
    "        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    if edge_index.numel() > 0:  # Sort indices.\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    return [Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles), mol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4c8d9-1846-434d-bb84-06d8bdb18b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets (split):\n",
    "  #without hydrogen\n",
    "  train_dataset=[]\n",
    "  train_mol = []\n",
    "  for i in range(len(split['train']['Drug'])):\n",
    "    data, mol = from_smiles(split['train']['Drug'][i])\n",
    "    data.x = data.x.float()\n",
    "    data.y = torch.tensor(float(split['train']['Y'][i]))\n",
    "    train_dataset.append(data)\n",
    "    train_mol.append(mol)\n",
    "\n",
    "  test_dataset=[]\n",
    "  test_mol = []\n",
    "  for i in range(len(split['test']['Drug'])):\n",
    "    data, mol = from_smiles(split['test']['Drug'][i])\n",
    "    data.x = data.x.float()\n",
    "    data.y = torch.tensor(float(split['test']['Y'][i]))\n",
    "    test_dataset.append(data)\n",
    "    test_mol.append(mol)\n",
    "\n",
    "  valid_dataset=[]\n",
    "  valid_mol = []\n",
    "  for i in range(len(split['valid']['Drug'])):\n",
    "    data, mol = from_smiles(split['valid']['Drug'][i])\n",
    "    data.x = data.x.float()\n",
    "    data.y = torch.tensor(float(split['valid']['Y'][i]))\n",
    "    valid_dataset.append(data)\n",
    "    valid_mol.append(mol)\n",
    "\n",
    "  print(f'Train : {len(train_dataset)}, Test : {len(test_dataset)}, Valid : {len(valid_dataset)}')\n",
    "  return [train_dataset, test_dataset, valid_dataset, train_mol, test_mol, valid_mol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8122820-17e7-44b7-80ea-ead13414f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "enc ={}\n",
    "dec ={}\n",
    "cmp_dec = {}\n",
    "train_duration = {}\n",
    "cmp_train_duration = {}\n",
    "opt_epochs = {}\n",
    "cmp_opt_epochs = {}\n",
    "test_auroc = {}\n",
    "cmp_test_auroc = {}\n",
    "test_loss = {}\n",
    "cmp_test_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df1df-7f23-4bff-a4c7-659d89f33e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier, self).__init__()\n",
    "    self.lin = Linear(32,8)\n",
    "    self.lin2 = Linear(8,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.lin(x).relu()\n",
    "    x=self.lin2(x)\n",
    "    return x\n",
    "\n",
    "class classifier2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier2, self).__init__()\n",
    "    self.lin = Linear(32,16)\n",
    "    self.lin2 = Linear(16,8)\n",
    "    self.lin3 = Linear(8,4)\n",
    "    self.lin4 = Linear(4,2)\n",
    "    self.lin5 = Linear(2,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.lin(x).relu()\n",
    "    x=self.lin2(x).relu()\n",
    "    x=self.lin3(x).relu()\n",
    "    x=self.lin4(x).relu()\n",
    "    x=self.lin5(x)\n",
    "    return x\n",
    "\n",
    "class classifier3(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier3, self).__init__()\n",
    "    self.lin = Linear(2048,2)\n",
    "    self.lin2 = Linear(2,2)\n",
    "    self.lin3 = Linear(2,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.lin(x).relu()\n",
    "    x=self.lin2(x).relu()\n",
    "    x=self.lin3(x)\n",
    "    return x\n",
    "\n",
    "class classifier4(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier4, self).__init__()\n",
    "    self.lin = Linear(167,24)\n",
    "    self.lin2 = Linear(24,2)\n",
    "    self.lin3 = Linear(2,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.lin(x).relu()\n",
    "    x=self.lin2(x).relu()\n",
    "    x=self.lin3(x)\n",
    "    return x\n",
    "\n",
    "class classifier5(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier5, self).__init__()\n",
    "    self.lin = Linear(167,4)\n",
    "    self.lin2 = Linear(4,4)\n",
    "    self.lin3 = Linear(4,4)\n",
    "    self.lin4 = Linear(4,2)\n",
    "    self.lin5 = Linear(2,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.lin(x).relu()\n",
    "    x=self.lin2(x).relu()\n",
    "    x=self.lin3(x).relu()\n",
    "    x=self.lin4(x).relu()\n",
    "    x=self.lin5(x)\n",
    "    return x\n",
    "\n",
    "class GCNencoder(nn.Module):\n",
    "  def __init__(self, nlayer=1, dropout=0.):\n",
    "    super(GCNencoder, self).__init__()\n",
    "    bn = nn.BatchNorm1d(32)\n",
    "    self.gcn = GCN(in_channels=9,\n",
    "                   hidden_channels = 32,\n",
    "                   num_layers = nlayer,\n",
    "                   norm=bn,\n",
    "                   dropout=0.\n",
    "                   )\n",
    "\n",
    "  def forward(self, x, edge_index, batch, edge_attr = None):\n",
    "    x = self.gcn(x, edge_index)\n",
    "    x = global_mean_pool(x, batch)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896ceb2-c721-4f8e-9fdb-f9135aa6fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_dataset(key_dataset):\n",
    "  global key, train_loader, valid_loader, test_loader, opt_epochs, test_auroc, test_loss, train_duration, cmp_opt_epochs, cmp_test_auroc, cmp_test_loss, cmp_train_duration, cmp_dec\n",
    "  key = key_dataset\n",
    "  train_loader = DataLoader(dataset[key]['train'], batch_size=batch_size, shuffle=True)\n",
    "  valid_loader = DataLoader(dataset[key]['valid'], batch_size=len(dataset[key]['valid']), shuffle=False)\n",
    "  test_loader = DataLoader(dataset[key]['test'], batch_size=len(dataset[key]['test']), shuffle=False)\n",
    "\n",
    "  enc[key]={}\n",
    "  dec[key]={}\n",
    "  opt_epochs[key] = {}\n",
    "  test_auroc[key] = {}\n",
    "  test_loss[key] = {}\n",
    "  train_duration[key] = {}\n",
    "\n",
    "  cmp_dec[key]={}\n",
    "  cmp_opt_epochs[key] = {}\n",
    "  cmp_test_auroc[key] = {}\n",
    "  cmp_test_loss[key] = {}\n",
    "  cmp_train_duration[key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de279da-abfb-4b1f-b8d3-5fde964e11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_cmp_dataset(cmp_key_dataset, reset=False):\n",
    "  global cmp_key, cmp_train_loader, cmp_valid_loader, cmp_test_loader, cmp_opt_epochs, test_auroc, test_loss\n",
    "  global de, optimizer, loss_function, train_loss, valid_loss, train_auroc, valid_auroc, cmp_train_duration, batch_size\n",
    "\n",
    "  cmp_key = cmp_key_dataset\n",
    "  batch_size =1024\n",
    "\n",
    "  if reset is False:\n",
    "    en = enc[key][nlayer].to(device)\n",
    "\n",
    "    ldr = DataLoader(dataset[cmp_key]['train'], batch_size=len(dataset[cmp_key]['train']), shuffle=False)\n",
    "    data = next(iter(ldr)).to(device)\n",
    "    out = en(data.x, data.edge_index, data.batch).detach()\n",
    "    cmp_train_loader = DataLoader(TensorDataset(out,data.y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ldr = DataLoader(dataset[cmp_key]['valid'], batch_size=len(dataset[cmp_key]['valid']), shuffle=False)\n",
    "    data = next(iter(ldr)).to(device)\n",
    "    out = en(data.x, data.edge_index, data.batch).detach()\n",
    "    out.y = data.y\n",
    "    cmp_valid_loader = DataLoader(TensorDataset(out,data.y), batch_size=out.shape[0], shuffle=False)\n",
    "\n",
    "    ldr = DataLoader(dataset[cmp_key]['test'], batch_size=len(dataset[cmp_key]['test']), shuffle=False)\n",
    "    data = next(iter(ldr)).to(device)\n",
    "    out = en(data.x, data.edge_index, data.batch).detach()\n",
    "    out.y = data.y\n",
    "    cmp_test_loader = DataLoader(TensorDataset(out,data.y), batch_size=out.shape[0], shuffle=False)\n",
    "\n",
    "    enc[key][nlayer] = en.cpu()\n",
    "    en = enc[key][nlayer]\n",
    "\n",
    "  de = classifier2().to(device)\n",
    "  loss_function = logistic_loss\n",
    "\n",
    "  train_loss = []\n",
    "  valid_loss = []\n",
    "  train_auroc = []\n",
    "  valid_auroc = []\n",
    "  cmp_train_duration[key][nlayer][cmp_key] = 0\n",
    "\n",
    "  print(f'# of parameters in decoder : {sum(p.numel() for p in de.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81197ad8-29bd-48fa-a5e4-65bd75035857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_fp(fp_key_in, cmp_key_dataset, reset=False):\n",
    "  global fp_key, cmp_key, cmp_train_loader, cmp_valid_loader, cmp_test_loader\n",
    "  global de, optimizer, loss_function, train_loss, valid_loss, train_auroc, valid_auroc,batch_size\n",
    "\n",
    "  fp_key = fp_key_in\n",
    "  cmp_key = cmp_key_dataset\n",
    "  batch_size =1024\n",
    "\n",
    "  fpgen = {'top':AllChem.GetRDKitFPGenerator(fpSize=2048),\n",
    "           'ap':AllChem.GetAtomPairGenerator(fpSize=2048),\n",
    "           'tt':AllChem.GetTopologicalTorsionGenerator(fpSize=2048),\n",
    "           'morgan':AllChem.GetMorganGenerator(fpSize=2048),\n",
    "           }.get(fp_key, None)\n",
    "  if fpgen is not None:\n",
    "    func = fpgen.GetFingerprint\n",
    "    de = classifier3().to(device)\n",
    "  elif fp_key == 'maccs':\n",
    "    func = MACCSkeys.GenMACCSKeys\n",
    "    de = classifier5().to(device)\n",
    "\n",
    "\n",
    "\n",
    "  if reset is False:\n",
    "    out = torch.tensor([(func(m)) for m in mol[cmp_key]['train']]).float()\n",
    "    lbls = torch.tensor([data.y for data in dataset[cmp_key]['train']])\n",
    "    cmp_train_loader = DataLoader(TensorDataset(out, lbls), batch_size = batch_size, shuffle=True)\n",
    "\n",
    "    out = torch.tensor([(func(m)) for m in mol[cmp_key]['valid']]).float()\n",
    "    lbls = torch.tensor([data.y for data in dataset[cmp_key]['valid']])\n",
    "    cmp_valid_loader = DataLoader(TensorDataset(out, lbls), batch_size = len(mol[cmp_key]['valid']), shuffle=False)\n",
    "\n",
    "    out = torch.tensor([(func(m)) for m in mol[cmp_key]['test']]).float()\n",
    "    lbls = torch.tensor([data.y for data in dataset[cmp_key]['test']])\n",
    "    cmp_test_loader = DataLoader(TensorDataset(out, lbls), batch_size = len(mol[cmp_key]['test']), shuffle=False)\n",
    "\n",
    "\n",
    "  loss_function = logistic_loss\n",
    "\n",
    "  train_loss = []\n",
    "  valid_loss = []\n",
    "  train_auroc = []\n",
    "  valid_auroc = []\n",
    "\n",
    "  print(f'# of parameters in decoder : {sum(p.numel() for p in de.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fe56e-0a5f-427e-8bc2-d3031fbe86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_nlayer(n=None, dropout=0.):\n",
    "  global batch_size, en, de, nlayer, optimizer, loss_function, train_loss, valid_loss, train_auroc, valid_auroc, train_duration\n",
    "  if n is not None:\n",
    "    nlayer = n\n",
    "  else:\n",
    "    n = nlayer\n",
    "  en = GCNencoder(nlayer=n, dropout=dropout).to(device)\n",
    "  de = classifier().to(device)\n",
    "  loss_function = logistic_loss\n",
    "  batch_size = 1024\n",
    "\n",
    "  train_loss = []\n",
    "  valid_loss = []\n",
    "  train_auroc = []\n",
    "  valid_auroc = []\n",
    "  train_duration[key][n] = 0\n",
    "\n",
    "  cmp_dec[key][n]={}\n",
    "  cmp_opt_epochs[key][n] = {}\n",
    "  cmp_test_auroc[key][n] = {}\n",
    "  cmp_test_loss[key][n] = {}\n",
    "  cmp_train_duration[key][n] = {}\n",
    "\n",
    "  print(f'# of parameters in encoder : {sum(p.numel() for p in en.parameters())}')\n",
    "  print(f'# of parameters in decoder : {sum(p.numel() for p in de.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c073be-63c9-462a-918c-d62c683c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_auroc(epochs, lr_val = None, cont = False, load_dir = None):\n",
    "  global lr, train_loss, valid_loss, train_auroc, valid_auroc, optimizer, final_state, en, de\n",
    "  if lr_val is not None:\n",
    "    lr = lr_val\n",
    "\n",
    "  if load_dir is not None:\n",
    "    en.load_state_dict(torch.load('encoder_state_'+key+'_'+str(nlayer)+'_ '+str(load_dir)+'.pt'))\n",
    "    de.load_state_dict(torch.load('decoder_state_'+key+'_'+str(nlayer)+'_ '+str(load_dir)+'.pt'))\n",
    "    optimizer.load_state_dict(torch.load('optimizer_state_'+key+'_'+str(nlayer)+'_ '+str(load_dir)+'.pt'))\n",
    "    [train_loss, train_auroc, valid_loss, valid_auroc] = torch.load('training mem_'+key+'_'+str(nlayer)+'_ '+str(load_dir)+'.pt')\n",
    "\n",
    "  else:\n",
    "    if cont is False:\n",
    "      optimizer = optim.Adam(list(en.parameters())+list(de.parameters()), lr=lr)\n",
    "    else:\n",
    "      en.load_state_dict(final_state[0])\n",
    "      de.load_state_dict(final_state[1])\n",
    "  valid_loss_min = 100\n",
    "  printed_valid_loss_min = 100\n",
    "  en = en.to(device)\n",
    "  de = de.to(device)\n",
    "  start=time()\n",
    "  for it in range(epochs):\n",
    "    en.train()\n",
    "    de.train()\n",
    "    loss_sum = 0\n",
    "    auroc_sum = 0\n",
    "    train_len = 0\n",
    "    for data in train_loader:\n",
    "      data = data.to(device)\n",
    "      out = de(en(data.x, data.edge_index, data.batch))\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_function(out, data.y.view(-1,1)).sum()\n",
    "      loss_sum += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      auroc_sum += binary_auroc(out.view(-1), data.y).item() * data.y.shape[0]\n",
    "      train_len += data.y.shape[0]\n",
    "    train_loss.append(loss_sum/train_len)\n",
    "    train_auroc.append(auroc_sum/train_len)\n",
    "\n",
    "    en.eval()\n",
    "    de.eval()\n",
    "    data = next(iter(valid_loader)).to(device)\n",
    "    out = de(en(data.x, data.edge_index, data.batch))\n",
    "    loss = loss_function(out, data.y.view(-1,1)).sum()\n",
    "    loss = loss.item()/data.y.shape[0]\n",
    "\n",
    "    if it == 0:\n",
    "      print(f'Estimated training time : {(time()-start)*epochs} secs.')\n",
    "\n",
    "    #print(valid_loss_min, printed_valid_loss_min, loss)\n",
    "    if valid_loss_min > loss:\n",
    "      if loss<0.6 and printed_valid_loss_min*0.95 > loss:\n",
    "        printed_valid_loss_min = loss\n",
    "        print(f'Find new local minima of valid loss : {loss : .3f}')\n",
    "      valid_loss_min = loss\n",
    "      save_state = [deepcopy(en.state_dict()), deepcopy(de.state_dict())]\n",
    "\n",
    "    valid_loss.append(loss)\n",
    "    valid_auroc.append(binary_auroc(out.view(-1), data.y).item())\n",
    "\n",
    "  train_duration[key][nlayer] += time()-start\n",
    "  final_state=[deepcopy(en.state_dict()), deepcopy(de.state_dict())]\n",
    "  en.load_state_dict(save_state[0])\n",
    "  de.load_state_dict(save_state[1])\n",
    "  plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67c659-ae01-4196-80ea-b53cdad7f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_train_loss_auroc(epochs, lr_val = None, cont=False):\n",
    "  global train_loss, valid_loss, train_auroc, valid_auroc, lr, optimizer, final_state, de\n",
    "  if lr_val is not None:\n",
    "    lr = lr_val\n",
    "  if cont is False:\n",
    "    optimizer = optim.Adam(list(de.parameters()), lr=lr)\n",
    "  else:\n",
    "    de.load_state_dict(final_state)\n",
    "  valid_loss_min = 100\n",
    "  printed_valid_loss_min = 100\n",
    "  de = de.to(device)\n",
    "  start=time()\n",
    "  for it in range(epochs):\n",
    "    de.train()\n",
    "    loss_sum = 0\n",
    "    auroc_sum = 0\n",
    "    train_len = 0\n",
    "    for data, label in cmp_train_loader:\n",
    "      data = data.to(device)\n",
    "      label = label.to(device)\n",
    "      out = de(data)\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_function(out, label.view(-1,1)).sum()\n",
    "      loss_sum += loss.item()\n",
    "      loss.backward() #retain_graph=True needed?\n",
    "      optimizer.step()\n",
    "      auroc_sum += binary_auroc(out.view(-1), label).item() * label.shape[0]\n",
    "      train_len += label.shape[0]\n",
    "    train_loss.append(loss_sum/train_len)\n",
    "    train_auroc.append(auroc_sum/train_len)\n",
    "\n",
    "    de.eval()\n",
    "    data, label = next(iter(cmp_valid_loader))\n",
    "    data, label = data.to(device), label.to(device)\n",
    "    out = de(data)\n",
    "    loss = loss_function(out, label.view(-1,1)).sum()\n",
    "    loss = loss.item()/label.shape[0]\n",
    "\n",
    "    if it == 0:\n",
    "      print(f'Estimated training time : {(time()-start)*epochs} secs.')\n",
    "\n",
    "    if valid_loss_min > loss:\n",
    "      if loss<0.6 and printed_valid_loss_min*0.9 > loss:\n",
    "        printed_valid_loss_min = loss\n",
    "        print(f'Find new local minima of valid loss : {loss : .3f}')\n",
    "      valid_loss_min = loss\n",
    "      save_state = deepcopy(de.state_dict())\n",
    "    valid_loss.append(loss)\n",
    "    valid_auroc.append(binary_auroc(out.view(-1), label).item())\n",
    "\n",
    "  final_state=deepcopy(de.state_dict())\n",
    "  de.load_state_dict(save_state)\n",
    "  plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4880f40-4e60-4b70-8209-408df27c0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_auroc():\n",
    "  fig, ax1 = plt.subplots()\n",
    "\n",
    "  color = 'tab:red'\n",
    "  ax1.set_xlabel('epochs')\n",
    "  ax1.set_ylabel('Mean Loss', color=color)\n",
    "  l1 = ax1.plot(range(len(train_loss)), train_loss, color = color, label='Train set-loss')\n",
    "  l2 = ax1.plot(range(len(valid_loss)), valid_loss, color = 'tab:orange', linestyle='dashed', label='Valid set-loss')\n",
    "  ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  color='tab:blue'\n",
    "  ax2.set_ylabel('AUROC', color=color)\n",
    "  l3 = ax2.plot(range(len(train_auroc)), train_auroc, color = color, label='Train set-AUROC')\n",
    "  l4 = ax2.plot(range(len(valid_auroc)), valid_auroc, color = 'tab:cyan', linestyle='dashed', label='Valid set-AUROC')\n",
    "  ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "  lns = l1 + l2 + l3 + l4\n",
    "  labs = [l.get_label() for l in lns]\n",
    "  ax1.legend(lns, labs, loc=7)\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d7fff-abff-4505-999b-77cdd603642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nlayer_auroc():\n",
    "  nlayers = [i for i in enc[key]]\n",
    "  plt.plot(nlayers, [test_auroc[key][i] for i in nlayers], color='black', label = key)\n",
    "  plt.xlabel('# of GCN layers')\n",
    "  plt.xticks(range(min(nlayers), max(nlayers)+1, 1))\n",
    "  plt.ylabel('AUROC of the embedding trained with '+key)\n",
    "  for k in ['HIV', 'HERG', 'CYP', 'TOX', 'BBB', 'HIA']:\n",
    "    nlayerst = []\n",
    "    for i in nlayers:\n",
    "      if k in cmp_test_auroc[key][i]:\n",
    "        nlayerst.append(i)\n",
    "    if len(nlayerst)>0:\n",
    "      plt.plot(nlayerst, [cmp_test_auroc[key][i][k] for i in nlayerst], linestyle='dashed', label='cmp: '+k)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc85c7-b6f6-4b18-b6c4-056e80a6feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval():\n",
    "  global test_loss, test_auroc, enc, dec\n",
    "  en.eval()\n",
    "  de.eval()\n",
    "  data = next(iter(test_loader)).to(device)\n",
    "  out = de(en(data.x, data.edge_index, data.batch))\n",
    "  test_loss[key][nlayer] = loss_function(out, data.y.view(-1,1)).mean().item()\n",
    "  test_auroc[key][nlayer] = binary_auroc(out.view(-1), data.y).item()\n",
    "  enc[key][nlayer] = en.cpu()\n",
    "  dec[key][nlayer] = de.cpu()\n",
    "  torch.save(enc[key][nlayer].state_dict(), 'encoder_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  files.download('encoder_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  torch.save(dec[key][nlayer].state_dict(), 'decoder_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  files.download('decoder_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  torch.save(optimizer.state_dict(), 'optimizer_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  files.download('optimizer_state_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "\n",
    "  torch.save([train_loss, train_auroc, valid_loss, valid_auroc], 'training mem_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "  files.download('training mem_'+key+'_'+str(nlayer)+'_'+f'{test_auroc[key][nlayer] : .3f}'+'.pt')\n",
    "\n",
    "  print(f'Test loss : {test_loss[key][nlayer] : .3f}, Test auroc : {test_auroc[key][nlayer] : .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29200022-bbd6-4338-8f1e-ad41ad45c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_get_eval():\n",
    "  global de\n",
    "  de.eval()\n",
    "  data, label = next(iter(cmp_test_loader))\n",
    "  data, label = data.to(device), label.to(device)\n",
    "  out = de(data)\n",
    "  loss = loss_function(out, label.view(-1,1)).mean().item()\n",
    "  auroc = binary_auroc(out.view(-1), label).item()\n",
    "\n",
    "  de = de.cpu()\n",
    "  torch.save(de, 'decoder_'+fp_key+'_to_'+cmp_key+'_'+f'{auroc : .3f}'+'.pt')\n",
    "  files.download('decoder_'+fp_key+'_to_'+cmp_key+'_'+f'{auroc : .3f}'+'.pt')\n",
    "\n",
    "  torch.save([train_loss, train_auroc, valid_loss, valid_auroc], 'training mem_'+fp_key+'_to_'+cmp_key+'_'+f'{auroc : .3f}'+'.pt')\n",
    "  files.download('training mem_'+fp_key+'_to_'+cmp_key+'_'+f'{auroc : .3f}'+'.pt')\n",
    "\n",
    "  print(f'Cmp Test loss : {loss : .3f}, Cmp Test auroc : {auroc : .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd3873-9e22-4838-84ac-975443150a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_get_eval():\n",
    "  global cmp_test_loss, cmp_test_auroc\n",
    "  de.eval()\n",
    "  data, label = next(iter(cmp_test_loader))\n",
    "  data, label = data.to(device), label.to(device)\n",
    "  out = de(data)\n",
    "  cmp_test_loss[key][nlayer][cmp_key] = loss_function(out, label.view(-1,1)).mean().item()\n",
    "  cmp_test_auroc[key][nlayer][cmp_key] = binary_auroc(out.view(-1), label).item()\n",
    "\n",
    "  cmp_dec[key][nlayer][cmp_key] = de.cpu()\n",
    "  torch.save(cmp_dec[key][nlayer][cmp_key], 'decoder_'+key+'_'+str(nlayer)+'_to_'+cmp_key+'_'+f'{cmp_test_auroc[key][nlayer][cmp_key] : .3f}'+'.pt')\n",
    "  files.download('decoder_'+key+'_'+str(nlayer)+'_to_'+cmp_key+'_'+f'{cmp_test_auroc[key][nlayer][cmp_key] : .3f}'+'.pt')\n",
    "\n",
    "  torch.save([train_loss, train_auroc, valid_loss, valid_auroc], 'training mem_'+key+'_'+str(nlayer)+'_to_'+cmp_key+'_'+f'{cmp_test_auroc[key][nlayer][cmp_key] : .3f}'+'.pt')\n",
    "  files.download('training mem_'+key+'_'+str(nlayer)+'_to_'+cmp_key+'_'+f'{cmp_test_auroc[key][nlayer][cmp_key] : .3f}'+'.pt')\n",
    "\n",
    "  print(f'Cmp Test loss : {cmp_test_loss[key][nlayer][cmp_key] : .3f}, Cmp Test auroc : {cmp_test_auroc[key][nlayer][cmp_key] : .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3d3fc-0d98-48bb-b4c6-2f46b678fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_distance(key_val, data_type, fp_type, emb_metric='euclidean'):\n",
    "  #Should set en before calling this function\n",
    "  global key, fp, d_fp, d_emb, d_arr_fp, darr_fp00, darr_fp01, darr_fp11, darr_emb, darr_emb00, darr_emb01, darr_emb11\n",
    "  key = key_val\n",
    "  fpgen = {'top':AllChem.GetRDKitFPGenerator(fpSize=2048),\n",
    "           'ap':AllChem.GetAtomPairGenerator(fpSize=2048),\n",
    "           'tt':AllChem.GetTopologicalTorsionGenerator(fpSize=2048),\n",
    "           'morgan':AllChem.GetMorganGenerator(fpSize=2048),\n",
    "           }.get(fp_type, None)\n",
    "  if fpgen is not None:\n",
    "    fp = [fpgen.GetFingerprint(m) for m in mol[key][data_type]]\n",
    "  elif fp_type == 'maccs':\n",
    "    fp = [MACCSkeys.GenMACCSKeys(m) for m in mol[key][data_type]]\n",
    "  else:\n",
    "    fp = None\n",
    "  darr_fp = []\n",
    "  for i in range(len(fp)):\n",
    "    darr_fp += [1-DataStructs.TanimotoSimilarity(fp[i], fp[j]) for j in range(i+1, len(fp))]\n",
    "  darr_fp = np.array(darr_fp)\n",
    "  d_fp = squareform(darr_fp)\n",
    "  #d_fp = np.matrix([[1-DataStructs.TanimotoSimilarity(fp[i], fp[j]) for j in range(i)] + [0]*(len(fp)-i) for i in range(len(fp))])\n",
    "  #d_fp += d_fp.T\n",
    "\n",
    "  en = en.to(device)\n",
    "  ldr = DataLoader(dataset[key][data_type], batch_size=len(dataset[key][data_type]), shuffle=False)\n",
    "  data = next(iter(ldr)).to(device)\n",
    "  out = enc(data.x, data.edge_index, data.batch)\n",
    "  darr_emb = pdist(out.detach.numpy(), metric=emb_metric)\n",
    "  d_emb = squareform(darr_emb)\n",
    "  #d_emb = np.matrix([[sum((out[i]-out[j])**2).item() for j in range(i)] + [0]*(out.shape[0]-i) for i in range(out.shape[0])])\n",
    "  #d_emb += d_emb.T\n",
    "\n",
    "  x,y = np.meshgrid(data.y, data.y)\n",
    "  d_label = np.matrix((x!=y).astype(int))\n",
    "\n",
    "  zero = np.where(data.y==0)\n",
    "  one = np.where(data.y==1)\n",
    "\n",
    "  x,y=np.meshgrid(zero,zero)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  darr_fp00 = np.squeeze(np.asarray(d_fp[(x,y)]))\n",
    "  darr_emb00 = np.squeeze(np.asarray(d_emb[(x,y)]))\n",
    "  x,y=np.meshgrid(zero,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  darr_fp01 = np.squeeze(np.asarray(d_fp[(x,y)]))\n",
    "  darr_emb01 = np.squeeze(np.asarray(d_emb[(x,y)]))\n",
    "  x,y=np.meshgrid(one,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  darr_fp11 = np.squeeze(np.asarray(d_fp[(x,y)]))\n",
    "  darr_emb11 = np.squeeze(np.asarray(d_emb[(x,y)]))\n",
    "\n",
    "  # darr_fp = np.squeeze(np.asarray(d_morgan[np.triu_indices(len(dataset[key][data_type]))]))\n",
    "  # darr_emb = np.squeeze(np.asarray(d_herg[np.triu_indices(len(dataset[key][data_type]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e523b-8c15-4a25-a0c6-5cc739a3e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HIV, HERG, CYP, TOX, BBB, HIA\n",
    "dataset={}\n",
    "mol={}\n",
    "for keys in ['HIV', 'HERG', 'CYP', 'TOX', 'BBB', 'HIA']:\n",
    "  dataset[keys]={}\n",
    "  mol[keys]={}\n",
    "#[dataset['HIV']['train'], dataset['HIV']['test'], dataset['HIV']['valid'], mol['HIV']['train'], mol['HIV']['test'], mol['HIV']['valid']] = prepare_datasets(HIV)\n",
    "# [dataset['HERG']['train'], dataset['HERG']['test'], dataset['HERG']['valid'], mol['HERG']['train'], mol['HERG']['test'], mol['HERG']['valid']] = prepare_datasets(HERG)\n",
    "[dataset['CYP']['train'], dataset['CYP']['test'], dataset['CYP']['valid'], mol['CYP']['train'], mol['CYP']['test'], mol['CYP']['valid']] = prepare_datasets(CYP)\n",
    "#[dataset['TOX']['train'], dataset['TOX']['test'], dataset['TOX']['valid'], mol['TOX']['train'], mol['TOX']['test'], mol['TOX']['valid']] = prepare_datasets(TOX)\n",
    "[dataset['BBB']['train'], dataset['BBB']['test'], dataset['BBB']['valid'], mol['BBB']['train'], mol['BBB']['test'], mol['BBB']['valid']] = prepare_datasets(BBB)\n",
    "[dataset['HIA']['train'], dataset['HIA']['test'], dataset['HIA']['valid'], mol['HIA']['train'], mol['HIA']['test'], mol['HIA']['valid']] = prepare_datasets(HIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8239e4-352d-45da-ad67-b0e0df789a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp={}\n",
    "d_fp = {}\n",
    "darr_fp = {}\n",
    "darr_fp00 = {}\n",
    "darr_fp01 = {}\n",
    "darr_fp11 = {}\n",
    "d_emb = {}\n",
    "darr_emb = {}\n",
    "darr_emb00 = {}\n",
    "darr_emb01 = {}\n",
    "darr_emb11 = {}\n",
    "d_label = {}\n",
    "#for k in ['CYP','BBB','HIA']:\n",
    "for k in ['CYP']:\n",
    "  fp[k]={}\n",
    "  d_fp[k]={}\n",
    "  darr_fp[k]={}\n",
    "  darr_fp00[k]={}\n",
    "  darr_fp01[k]={}\n",
    "  darr_fp11[k]={}\n",
    "\n",
    "\n",
    "\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    fpgen = {'top':AllChem.GetRDKitFPGenerator(fpSize=2048),\n",
    "            'ap':AllChem.GetAtomPairGenerator(fpSize=2048),\n",
    "            'tt':AllChem.GetTopologicalTorsionGenerator(fpSize=2048),\n",
    "            'morgan':AllChem.GetMorganGenerator(fpSize=2048),\n",
    "            }.get(fpk, None)\n",
    "    if fpk != 'maccs':\n",
    "      fps = [fpgen.GetFingerprint(m) for m in mol[k]['test']]\n",
    "    else:\n",
    "      fps = [MACCSkeys.GenMACCSKeys(m) for m in mol[k]['test']]\n",
    "    darr_fp[k][fpk] = []\n",
    "    for i in range(len(fps)):\n",
    "      darr_fp[k][fpk] += [1-DataStructs.TanimotoSimilarity(fps[i], fps[j]) for j in range(i+1, len(fps))]\n",
    "    darr_fp[k][fpk] = np.array(darr_fp[k][fpk])\n",
    "    d_fp[k][fpk] = squareform(darr_fp[k][fpk])\n",
    "\n",
    "  decide_dataset(k)\n",
    "  decide_nlayer({'CYP' : 4, 'BBB' :6, 'HIA':7}.get(k, None))\n",
    "  en.load_state_dict(torch.load({\n",
    "      'CYP' : 'encoder_state_CYP_4_ 0.847.pt',\n",
    "      'BBB' : 'encoder_state_BBB_6_ 0.871.pt',\n",
    "      'HIA' : 'encoder_state_HIA_7_ 0.973.pt'\n",
    "  }.get(k,None)))\n",
    "  en = en.to(device)\n",
    "  ldr = DataLoader(dataset[k]['test'], batch_size=len(dataset[k]['test']), shuffle=False)\n",
    "  data = next(iter(ldr)).to(device)\n",
    "  out = en(data.x, data.edge_index, data.batch)\n",
    "  darr_emb[k] = pdist(out.detach().numpy(), metric='euclidean')\n",
    "  d_emb[k] = squareform(darr_emb[k])\n",
    "\n",
    "  x,y = np.meshgrid(data.y, data.y)\n",
    "  d_label[k] = np.matrix((x!=y).astype(int))\n",
    "\n",
    "  zero = np.where(data.y==0)\n",
    "  one = np.where(data.y==1)\n",
    "\n",
    "  x,y=np.meshgrid(zero,zero)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    darr_fp00[k][fpk] = np.squeeze(np.asarray(d_fp[k][fpk][(x,y)]))\n",
    "  darr_emb00[k] = np.squeeze(np.asarray(d_emb[k][(x,y)]))\n",
    "\n",
    "  x,y=np.meshgrid(zero,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    darr_fp01[k][fpk] = np.squeeze(np.asarray(d_fp[k][fpk][(x,y)]))\n",
    "  darr_emb01[k] = np.squeeze(np.asarray(d_emb[k][(x,y)]))\n",
    "\n",
    "  x,y=np.meshgrid(one,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    darr_fp11[k][fpk] = np.squeeze(np.asarray(d_fp[k][fpk][(x,y)]))\n",
    "  darr_emb11[k] = np.squeeze(np.asarray(d_emb[k][(x,y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4b300-640e-432d-9f21-95c25157ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def khist(k):\n",
    "  xname = 'Distance'\n",
    "  lname = 'Embedding w/ '+k\n",
    "  data = {\n",
    "      xname : np.concatenate([darr_emb00[k],darr_emb11[k],darr_emb01[k]]),\n",
    "      lname : ['same label']*(len(darr_emb00[k]) + len(darr_emb11[k])) + ['different label']* len(darr_emb01[k])\n",
    "  }\n",
    "  df = pd.DataFrame(data)\n",
    "  df=df.replace(.0,np.nan)\n",
    "  plt.figure()\n",
    "  ax = sns.displot(df, x= xname,hue=lname,kind=\"kde\",common_norm=False)\n",
    "  sns.move_legend(ax, 'center right')\n",
    "  plt.axvline((np.concatenate([darr_emb00[k],darr_emb11[k]])).mean(),c='blue',ls=':',lw=2.5)\n",
    "  plt.axvline((darr_emb01[k]).mean(),c='orange',ls=':',lw=2.5)\n",
    "  plt.savefig('khist_'+k+'.png')\n",
    "  plt.close()\n",
    "  files.download('khist_'+k+'.png')\n",
    "\n",
    "def fphist(k,fpk):\n",
    "  xname = 'Distance'\n",
    "  lname = fpk+' fingerprint w/ '+k\n",
    "  data = {\n",
    "      xname : np.concatenate([darr_fp00[k][fpk],darr_fp11[k][fpk],darr_fp01[k][fpk]]),\n",
    "      lname : ['same label']*(len(darr_fp00[k][fpk]) + len(darr_fp11[k][fpk])) + ['different label']* len(darr_fp01[k][fpk])\n",
    "  }\n",
    "  df = pd.DataFrame(data)\n",
    "  df=df.replace(.0,np.nan)\n",
    "  plt.figure()\n",
    "  ax = sns.displot(df, x= xname,hue=lname,kind=\"kde\",common_norm=False)\n",
    "  sns.move_legend(ax, 'center right')\n",
    "  plt.axvline((np.concatenate([darr_fp00[k][fpk],darr_fp11[k][fpk]])).mean(),c='blue',ls=':',lw=2.5)\n",
    "  plt.axvline((darr_fp01[k][fpk]).mean(),c='orange',ls=':',lw=2.5)\n",
    "  plt.savefig('fphist_'+k+'_'+fpk+'.png')\n",
    "  plt.close()\n",
    "  files.download('fphist_'+k+'_'+fpk+'.png')\n",
    "\n",
    "def dcorrplot(k,fpk,fpk2=None):\n",
    "  if fpk2 is None:\n",
    "    xname = 'Distance from embedding trained on '+k\n",
    "    yname = 'Distance from '+fpk+' fingerprint'\n",
    "    lname = k\n",
    "    logpair = (False,False)\n",
    "    savename = 'dcorrplot_'+k+'_'+fpk+'.png'\n",
    "    data = {\n",
    "        xname : np.concatenate([darr_emb00[k],darr_emb11[k],darr_emb01[k]]),\n",
    "        yname : np.concatenate([darr_fp00[k][fpk],darr_fp11[k][fpk],darr_fp01[k][fpk]]),\n",
    "        lname : ['same label']*(len(darr_fp00[k][fpk]) + len(darr_fp11[k][fpk])) + ['different label']* len(darr_fp01[k][fpk])\n",
    "    }\n",
    "  else:\n",
    "    xname = 'Distance from '+fpk+' fingerprint'\n",
    "    yname = 'Distance from '+fpk2+' fingerprint'\n",
    "    lname = k\n",
    "    logpair=None\n",
    "    savename = 'dcorrplot of two fps_'+k+'_'+fpk+'_'+fpk2+'.png'\n",
    "    data = {\n",
    "        xname : np.concatenate([darr_fp00[k][fpk],darr_fp11[k][fpk],darr_fp01[k][fpk]]),\n",
    "        yname : np.concatenate([darr_fp00[k][fpk2],darr_fp11[k][fpk2],darr_fp01[k][fpk2]]),\n",
    "        lname : ['same label']*(len(darr_fp00[k][fpk]) + len(darr_fp11[k][fpk])) + ['different label']* len(darr_fp01[k][fpk])\n",
    "    }\n",
    "  df = pd.DataFrame(data)\n",
    "  df=df.replace(.0,np.nan)\n",
    "  plt.figure()\n",
    "  ax = sns.displot(df, x= xname,y=yname,hue=lname,kind=\"kde\",rug=True,rug_kws={\"alpha\" : 0.01},common_norm=False,fill=True,levels=15,alpha=.5,log_scale=logpair)\n",
    "  sns.move_legend(ax, 'center right')\n",
    "  plt.savefig(savename)\n",
    "  plt.close()\n",
    "  files.download(savename)\n",
    "\n",
    "dcorr = Dcorr(compute_distance=None)\n",
    "mgc = MGC(compute_distance=None)\n",
    "def getstat(k, fpk=None, fpk2=None):\n",
    "  if fpk is None:\n",
    "    d1 =d_emb[k]\n",
    "    d2 = d_label[k]\n",
    "  elif fpk2 is None:\n",
    "    d1 = d_emb[k]\n",
    "    d2 = d_fp[k][fpk]\n",
    "    savename = 'MGC_'+k+'_'+fpk+'.png'\n",
    "  else:\n",
    "    d1 = d_fp[k][fpk]\n",
    "    d2 = d_fp[k][fpk2]\n",
    "    savename = 'MGC of two fps_'+k+'_'+fpk+'_'+fpk2+'.png'\n",
    "\n",
    "  stat,pvalue = dcorr.test(d1, d2)\n",
    "  print('[Dcorr] stat : %.5f, pvalue : %.5f'%(stat,pvalue))\n",
    "\n",
    "  if fpk is not None:\n",
    "    stat, pvalue, mgc_dict = mgc.test(d1,d2, reps=0)\n",
    "    print('[MGC] stat : %.5f, pvalue : %.5f'%(stat,pvalue))\n",
    "\n",
    "    plt.figure()\n",
    "    # make plots look pretty\n",
    "    sns.set(color_codes=True, style=\"white\", context=\"talk\", font_scale=1)\n",
    "\n",
    "    mgc_map = mgc_dict[\"mgc_map\"]\n",
    "    opt_scale = mgc_dict[\"opt_scale\"]  # i.e. maximum smoothed test statistic\n",
    "    print(\"Optimal Scale:\", opt_scale)\n",
    "\n",
    "    # create figure\n",
    "    fig, (ax, cax) = plt.subplots(\n",
    "        ncols=2, figsize=(9, 8.5), gridspec_kw={\"width_ratios\": [1, 0.05]}\n",
    "    )\n",
    "\n",
    "    # draw heatmap and colorbar\n",
    "    ax = sns.heatmap(mgc_map, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "    fig.colorbar(ax.get_children()[0], cax=cax, orientation=\"vertical\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # optimal scale\n",
    "    ax.scatter(opt_scale[1], opt_scale[0], marker=\"X\", s=200, color=\"red\")\n",
    "\n",
    "    # make plots look nice\n",
    "    ax.set_title(\"MGC Map\")\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    ax.set_xlabel(\"Neighbors for x\")\n",
    "    ax.set_ylabel(\"Neighbors for y\")\n",
    "    N=d_emb[k].shape[0]\n",
    "    ax.set_xticks([0, N/2, N])\n",
    "    ax.set_yticks([0, N/2, N])\n",
    "    ax.xaxis.set_tick_params()\n",
    "    ax.yaxis.set_tick_params()\n",
    "    cax.xaxis.set_tick_params()\n",
    "    cax.yaxis.set_tick_params()\n",
    "\n",
    "    plt.savefig(savename)\n",
    "    plt.close()\n",
    "    files.download(savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63c0f9-37bf-4bcc-801a-f536a5e0f040",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c354a-d03c-4907-9dff-27a5af37693f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50c237-fb6e-43dd-afbc-f3c87c2e2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_dataset('HERG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4dc6b-aba9-452a-8820-bf378fe9004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_nlayer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46ea2a-0159-47dd-9799-2c7734ce6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_auroc(10,0.01, True)\n",
    "plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60277562-7e69-4476-8939-5bddd1f72db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168c59-1ffb-4dd5-b77f-0f50f95ef065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also upload en, de that was saved at get_eval()\n",
    "en=torch.load('encoder_HERG_3.pt')\n",
    "de=torch.load('decoder_HERG_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df7ecd-5256-4363-ba26-1985f4117ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_auroc(10,0.01, True)\n",
    "plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f0b9f-7aac-4f2d-a973-3d1f16f55133",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa117bdd-02a1-4776-af22-c609df256a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_nlayer(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a00cf-4815-48ed-8cf0-57bfc8b3983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_auroc(10,0.01, True)\n",
    "plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac99a4-a9ed-465a-9cee-e33a18de1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7a128-6af2-4f34-99ac-e44990174c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_dataset('HIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c189379-7073-455e-b237-cd8868237f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_nlayer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5ad37-a38b-423a-8763-578d32fa0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_auroc(10,0.01, True)\n",
    "plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685b804-2e6a-4aac-a395-3d5c2dea65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d151eb-93ec-4722-a289-19aa5db510a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train decoder (with pre-trained encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34db391-e26b-4010-93f3-6ba93f781ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_cmp_dataset('BBB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889111bc-f237-4b1a-b8e1-0d2a38e46f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_train_loss_auroc(1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91f0e0-0ed4-4ba2-943a-a3b842c2b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_auroc()\n",
    "cmp_opt_epochs[key][nlayer][cmp_key] = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e49650-ed25-44a6-96df-82b1e60da865",
   "metadata": {},
   "source": [
    "You can re-train after getting the optimal epoch to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784dc77-914e-4049-a913-722d22f45455",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_cmp_dataset('BBB', reset=True)\n",
    "cmp_train_loss_auroc(100,0.001)\n",
    "plot_loss_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af72c9e-d12f-4734-8044-892c56050d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_train_loss_auroc(cmp_opt_epochs[key][nlayer][cmp_key] -100, 0.001)\n",
    "plot_loss_auroc()\n",
    "cmp_get_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa55b57-67df-4dc6-b1a1-cd564afbade5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1eeb1e-40e5-452f-9824-1b42764717b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "khist('HIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c570ee-5a58-425b-98db-f1b29df11e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "getstat('HIA','maccs','top')\n",
    "getstat('HIA','maccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed105f91-1ffc-4b05-8892-bdf43d0bb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorrplot('BBB','top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989bce0-13cf-4818-8703-2c4b793c74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorrplot('HIA','top','maccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05837b9-2b5d-4595-9b1a-5ec4caef9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fphist('HIA','maccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0df6d8-a572-46d2-a2f4-b31f6d69d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(darr_emb00['HIA'],bins=50,alpha=1,label='00')\n",
    "plt.hist(darr_emb01['HIA'],bins=50,alpha=0.67,label='01')\n",
    "plt.hist(darr_emb11['HIA'],bins=50,alpha=0.33,label='11')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001148b-aca6-4c01-a902-b9fdacba7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'HIA'\n",
    "khist(k)\n",
    "for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "  fphist(k,fpk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe6b2f-9ce5-44c2-9754-84902d0a2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k='HIA'\n",
    "for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "  print(k, fpk)\n",
    "  dcorrplot(k,fpk)\n",
    "  getstat(k,fpk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd9274-c9ed-4c33-afb2-9836a8e63dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "getstat('HIA')\n",
    "getstat('BBB')\n",
    "getstat('CYP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c0cc8-30f0-476f-94d5-82dea7a0045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_dataset('CYP')\n",
    "ldr = DataLoader(dataset['CYP']['test'], batch_size=len(dataset['CYP']['test']), shuffle=False)\n",
    "data = next(iter(ldr)).to(device)\n",
    "darr_cyp={}\n",
    "darr_cyp00={}\n",
    "darr_cyp01={}\n",
    "darr_cyp11={}\n",
    "d_cyp = {}\n",
    "for n in [2,3,4,5,6,7,8,9,11,13,15,20,30]:\n",
    "  decide_nlayer(n)\n",
    "  tmp = {\n",
    "      2:'0.824',\n",
    "      3:'0.843',\n",
    "      4:'0.847',\n",
    "      5:'0.845',\n",
    "      6:'0.823',\n",
    "      7:'0.830',\n",
    "      8:'0.840',\n",
    "      9:'0.832',\n",
    "      11:'0.835',\n",
    "      13:'0.810',\n",
    "      15:'0.826',\n",
    "      20:'0.814',\n",
    "      30:'0.793'\n",
    "  }.get(n,None)\n",
    "  en.load_state_dict(torch.load('encoder_state_CYP_'+str(n)+'_ '+tmp+'.pt'))\n",
    "  en = en.to(device)\n",
    "  out = en(data.x, data.edge_index, data.batch)\n",
    "  darr_cyp[n] = pdist(out.detach().numpy(), metric='euclidean')\n",
    "  d_cyp[n] = squareform(darr_cyp[n])\n",
    "\n",
    "  zero = np.where(data.y==0)\n",
    "  one = np.where(data.y==1)\n",
    "\n",
    "  x,y=np.meshgrid(zero,zero)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  darr_cyp00[n] = np.squeeze(np.asarray(d_cyp[n][(x,y)]))\n",
    "\n",
    "  x,y=np.meshgrid(zero,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "  darr_cyp01[n] = np.squeeze(np.asarray(d_cyp[n][(x,y)]))\n",
    "\n",
    "  x,y=np.meshgrid(one,one)\n",
    "  x,y=x.flatten(), y.flatten()\n",
    "\n",
    "  darr_cyp11[n] = np.squeeze(np.asarray(d_cyp[n][(x,y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3ceb3-3ec9-44a6-9529-f2fedbd21b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [2,3,4,5,6,7,8,9,11,13,15,20,30]:\n",
    "  xname = 'Distance'\n",
    "  lname = 'Embedding w/ '+k+' and #layer = '+str(n)\n",
    "  data = {\n",
    "      xname : np.concatenate([darr_cyp00[n],darr_cyp11[n],darr_cyp01[n]]),\n",
    "      lname : ['same label']*(len(darr_cyp00[n]) + len(darr_cyp11[n])) + ['different label']* len(darr_cyp01[n])\n",
    "  }\n",
    "  df = pd.DataFrame(data)\n",
    "  df=df.replace(.0,np.nan)\n",
    "  plt.figure()\n",
    "  ax = sns.displot(df, x= xname,hue=lname,kind=\"kde\",common_norm=False)\n",
    "  sns.move_legend(ax, 'center right')\n",
    "  plt.axvline((np.concatenate([darr_cyp00[n],darr_cyp11[n]])).mean(),c='blue',ls=':',lw=2.5)\n",
    "  plt.axvline((darr_cyp01[n]).mean(),c='orange',ls=':',lw=2.5)\n",
    "  plt.savefig('khist_'+k+'_layer'+str(n)+'.png')\n",
    "  plt.close()\n",
    "  files.download('khist_'+k+'_layer'+str(n)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aef41-78ed-4bc9-8f74-6042eb4c263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [2,3,4,5,6,7,8,9,11,13,15,20,30]:\n",
    "  print('Layer #: ',n)\n",
    "  stat,pvalue = dcorr.test(d_cyp[n], d_label['CYP'])\n",
    "  print('[Dcorr-label] stat : %.5f, pvalue : %.5f'%(stat,pvalue))\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    print(fpk)\n",
    "    stat,pvalue = dcorr.test(d_cyp[n], d_fp['CYP'][fpk])\n",
    "    print('[Dcorr] stat : %.5f, pvalue : %.5f'%(stat,pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615c215-69b1-44a1-b9ae-e196037a09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [2,3,4,5,6,7,8,9,11,13,15,20,30]:\n",
    "  print('Layer #: ',n)\n",
    "  for fpk in ['top','ap','tt','morgan','maccs']:\n",
    "    savename = 'MGC_CYP_'+str(n)+'_'+fpk+'.png'\n",
    "    print(fpk)\n",
    "\n",
    "    stat, pvalue, mgc_dict = mgc.test(d_cyp[n], d_fp['CYP'][fpk], reps=10)\n",
    "    print('[MGC] stat : %.5f, pvalue : %.5f'%(stat,pvalue))\n",
    "\n",
    "    plt.figure()\n",
    "    # make plots look pretty\n",
    "    sns.set(color_codes=True, style=\"white\", context=\"talk\", font_scale=1)\n",
    "\n",
    "    mgc_map = mgc_dict[\"mgc_map\"]\n",
    "    opt_scale = mgc_dict[\"opt_scale\"]  # i.e. maximum smoothed test statistic\n",
    "    print(\"Optimal Scale:\", opt_scale)\n",
    "\n",
    "    # create figure\n",
    "    fig, (ax, cax) = plt.subplots(\n",
    "        ncols=2, figsize=(9, 8.5), gridspec_kw={\"width_ratios\": [1, 0.05]}\n",
    "    )\n",
    "\n",
    "    # draw heatmap and colorbar\n",
    "    ax = sns.heatmap(mgc_map, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "    fig.colorbar(ax.get_children()[0], cax=cax, orientation=\"vertical\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # optimal scale\n",
    "    ax.scatter(opt_scale[1], opt_scale[0], marker=\"X\", s=200, color=\"red\")\n",
    "\n",
    "    # make plots look nice\n",
    "    ax.set_title(\"MGC Map\")\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    ax.set_xlabel(\"Neighbors for x\")\n",
    "    ax.set_ylabel(\"Neighbors for y\")\n",
    "    N=d_emb[k].shape[0]\n",
    "    ax.set_xticks([0, N/2, N])\n",
    "    ax.set_yticks([0, N/2, N])\n",
    "    ax.xaxis.set_tick_params()\n",
    "    ax.yaxis.set_tick_params()\n",
    "    cax.xaxis.set_tick_params()\n",
    "    cax.yaxis.set_tick_params()\n",
    "\n",
    "    plt.savefig(savename)\n",
    "    plt.close()\n",
    "    files.download(savename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
